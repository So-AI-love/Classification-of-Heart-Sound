{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "deep_pcg.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njETfrsU1CgO"
      },
      "source": [
        "# Trying to install the Python modules and Add github Project files to Colab Drive and enviroment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdnG6nJ91Tcz",
        "outputId": "06b97c0c-6128-4dd3-b732-867de99fd254"
      },
      "source": [
        "!git clone https://github.com/tkseneee/Classification-of-Heart-Sound\n",
        "%cd Classification-of-Heart-Sound"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Classification-of-Heart-Sound'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Total 15 (delta 0), reused 0 (delta 0), pack-reused 15\u001b[K\n",
            "Unpacking objects: 100% (15/15), done.\n",
            "/content/Classification-of-Heart-Sound\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hclCZE8M1V3K"
      },
      "source": [
        "Importing the Database from Google Drive:\n",
        "\n",
        "First add this Files to your Google Drive by pressing this Icon via Bowser:\n",
        "\n",
        "\n",
        "https://drive.google.com/open?id=13ehWqXt8YDrmmjQc7XAUqcCk6Dwb69hy\n",
        "\n",
        "\n",
        "<img src=\"https://i.stack.imgur.com/TqOfU.png\" width=\"300\" />\n",
        "\n",
        "Connect ot your Google Drvie by this codes:\n",
        "```\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex9lH5DX4Wot",
        "outputId": "5228107d-e578-4e9c-b338-f7eac402d9e6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp -r /content/drive/MyDrive/Data.rar /content"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln3fL2Iy4eg1"
      },
      "source": [
        "## Unrar the Data.rar file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Afw3qMKt4jRl",
        "outputId": "00bc4d35-22f6-4f1c-aabd-96892c40d0aa"
      },
      "source": [
        "!sudo apt-get install unrar\n",
        "%cd /content\n",
        "!unrar e   /content/Data.rar"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "unrar is already the newest version (1:5.5.8-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
            "/content\n",
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from /content/Data.rar\n",
            "\n",
            "Extracting  set_a.csv                                                    \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Extracting  set_a.zip                                                    \b\b\b\b  0%\b\b\b\b  1%\b\b\b\b  2%\b\b\b\b  3%\b\b\b\b  4%\b\b\b\b  5%\b\b\b\b  6%\b\b\b\b  7%\b\b\b\b  8%\b\b\b\b  9%\b\b\b\b 10%\b\b\b\b 11%\b\b\b\b 12%\b\b\b\b 13%\b\b\b\b 14%\b\b\b\b 15%\b\b\b\b 16%\b\b\b\b 17%\b\b\b\b 18%\b\b\b\b 19%\b\b\b\b 20%\b\b\b\b 21%\b\b\b\b 22%\b\b\b\b 23%\b\b\b\b 24%\b\b\b\b 25%\b\b\b\b 26%\b\b\b\b 27%\b\b\b\b 28%\b\b\b\b 29%\b\b\b\b 30%\b\b\b\b 31%\b\b\b\b 32%\b\b\b\b 33%\b\b\b\b 34%\b\b\b\b 35%\b\b\b\b 36%\b\b\b\b 37%\b\b\b\b 38%\b\b\b\b 39%\b\b\b\b 40%\b\b\b\b 41%\b\b\b\b 42%\b\b\b\b 43%\b\b\b\b 44%\b\b\b\b 45%\b\b\b\b 46%\b\b\b\b 47%\b\b\b\b 48%\b\b\b\b 49%\b\b\b\b 50%\b\b\b\b 51%\b\b\b\b 52%\b\b\b\b 53%\b\b\b\b 54%\b\b\b\b 55%\b\b\b\b 56%\b\b\b\b 57%\b\b\b\b 58%\b\b\b\b 59%\b\b\b\b 60%\b\b\b\b 61%\b\b\b\b 62%\b\b\b\b 63%\b\b\b\b 64%\b\b\b\b 65%\b\b\b\b 66%\b\b\b\b 67%\b\b\b\b 68%\b\b\b\b 69%\b\b\b\b 70%\b\b\b\b 71%\b\b\b\b 72%\b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Extracting  set_b.csv                                                    \b\b\b\b 73%\b\b\b\b\b  OK \n",
            "Extracting  set_b.zip                                                    \b\b\b\b 77%\b\b\b\b 81%\b\b\b\b 84%\b\b\b\b 88%\b\b\b\b 92%\b\b\b\b 95%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ipibimi6PqA"
      },
      "source": [
        "### Now we have database files as you can see below :\n",
        "\n",
        "So the Input Direction woule be : `INPUT_DIR='/content/'`\n",
        "\n",
        "<img src=\"https://i.stack.imgur.com/ZFFsi.png\" width=\"300\" />\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Kkb3S2I74rs"
      },
      "source": [
        "!unzip -d set_a /content/set_a.zip\n",
        "!unzip -d set_b /content/set_b.zip\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poEel9y50-RZ"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import librosa"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njPKqRQt0-RZ"
      },
      "source": [
        "INPUT_DIR=\"/content\"  # comment \"C:\\\\Users\\\\senthilku\\\\Desktop\\\\Heart Sound\"\n",
        "SAMPLE_RATE = 16000\n",
        "MAX_SOUND_CLIP_DURATION=12 #sec  "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTGmznLa0-RZ"
      },
      "source": [
        "set_a=pd.read_csv(INPUT_DIR+\"/set_a.csv\")\n",
        "#set_a_timing=pd.read_csv(INPUT_DIR+\"/set_a_timing.csv\")\n",
        "set_b=pd.read_csv(INPUT_DIR+\"/set_b.csv\")\n",
        "frames = [set_a, set_b]\n",
        "data_ab=pd.concat(frames)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnuRHVvv0-RZ"
      },
      "source": [
        "def audio_norm(data):\n",
        "    max_data = np.max(data)\n",
        "    min_data = np.min(data)\n",
        "    data = (data-min_data)/(max_data-min_data+0.0001)\n",
        "    return data-0.5\n",
        "\n",
        "# get audio data without padding highest quality audio\n",
        "def load_file_data_without_change(folder,file_names, duration=3, sr=16000):\n",
        "    input_length=sr*duration\n",
        "    # function to load files and extract features\n",
        "    # file_names = glob.glob(os.path.join(folder, '*.wav'))\n",
        "    data = []\n",
        "    for file_name in file_names:\n",
        "        try:\n",
        "            sound_file=folder+file_name\n",
        "            print (\"load file \",sound_file)\n",
        "            # use kaiser_fast technique for faster extraction\n",
        "            X, sr = librosa.load( sound_file, res_type='kaiser_fast') \n",
        "            dur = librosa.get_duration(y=X, sr=sr)\n",
        "            # extract normalized mfcc feature from data\n",
        "            #mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sr, n_mfcc=40).T,axis=0) \n",
        "        except Exception as e:\n",
        "            print(\"Error encountered while parsing file: \", file)\n",
        "        #feature = np.array(mfccs).reshape([-1,1])\n",
        "        data.append(X)\n",
        "    return data\n",
        "\n",
        "\n",
        "# get audio data with a fix padding may also chop off some file\n",
        "def load_file_data (folder,file_names, duration=12, sr=16000):\n",
        "    input_length=sr*duration\n",
        "    # function to load files and extract features\n",
        "    # file_names = glob.glob(os.path.join(folder, '*.wav'))\n",
        "    data = []\n",
        "    for file_name in file_names:\n",
        "        try:\n",
        "            sound_file=folder+file_name\n",
        "            print (\"load file \",sound_file)\n",
        "            # use kaiser_fast technique for faster extraction\n",
        "            X, sr = librosa.load( sound_file, sr=sr, duration=duration,res_type='kaiser_fast') \n",
        "            dur = librosa.get_duration(y=X, sr=sr)\n",
        "            # pad audio file same duration\n",
        "            if (round(dur) < duration):\n",
        "                print (\"fixing audio lenght :\", file_name)\n",
        "                y = librosa.util.fix_length(X, input_length)                \n",
        "            #normalized raw audio \n",
        "            y = audio_norm(y)            \n",
        "            # extract normalized mfcc feature from data\n",
        "            #mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sr, n_mfcc=40).T,axis=0)             \n",
        "        except Exception as e:\n",
        "            print(\"Error encountered while parsing file: \", file)        \n",
        "        #feature = np.array(mfccs).reshape([-1,1])\n",
        "        data.append(y)\n",
        "    return data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Quq6Q0GS0-Ra",
        "outputId": "86587f63-1da0-4c7b-d4f6-5b82921f0bec"
      },
      "source": [
        "# simple encoding of categories, limited to 3 types\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Map label text to integer\n",
        "CLASSES = ['artifact','murmur','normal']\n",
        "# {'artifact': 0, 'murmur': 1, 'normal': 3}\n",
        "NB_CLASSES=len(CLASSES)\n",
        "\n",
        "# Map integer value to text labels\n",
        "label_to_int = {k:v for v,k in enumerate(CLASSES)}\n",
        "print (label_to_int)\n",
        "print (\" \")\n",
        "# map integer to label text\n",
        "int_to_label = {v:k for k,v in label_to_int.items()}\n",
        "print(int_to_label)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'artifact': 0, 'murmur': 1, 'normal': 2}\n",
            " \n",
            "{0: 'artifact', 1: 'murmur', 2: 'normal'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_iw_U5u0-Rb",
        "outputId": "ee671442-26df-4c91-87ae-f33e1e06b1ed"
      },
      "source": [
        "# load dataset-a, keep them separate for testing purpose\n",
        "import os, fnmatch\n",
        "\n",
        "A_folder=INPUT_DIR+'/set_a/'\n",
        "# set-a\n",
        "A_artifact_files = fnmatch.filter(os.listdir(INPUT_DIR+'/set_a'), 'artifact*.wav')\n",
        "A_artifact_sounds = load_file_data(folder=A_folder,file_names=A_artifact_files, duration=MAX_SOUND_CLIP_DURATION)\n",
        "A_artifact_labels = [0 for items in A_artifact_files]\n",
        "\n",
        "A_normal_files = fnmatch.filter(os.listdir(INPUT_DIR+'/set_a'), 'normal*.wav')\n",
        "A_normal_sounds = load_file_data(folder=A_folder,file_names=A_normal_files, duration=MAX_SOUND_CLIP_DURATION)\n",
        "A_normal_labels = [2 for items in A_normal_sounds]\n",
        "\n",
        "A_extrahls_files = fnmatch.filter(os.listdir(INPUT_DIR+'/set_a'), 'extrahls*.wav')\n",
        "A_extrahls_sounds = load_file_data(folder=A_folder,file_names=A_extrahls_files, duration=MAX_SOUND_CLIP_DURATION)\n",
        "A_extrahls_labels = [1 for items in A_extrahls_sounds]\n",
        "\n",
        "A_murmur_files = fnmatch.filter(os.listdir(INPUT_DIR+'/set_a'), 'murmur*.wav')\n",
        "A_murmur_sounds = load_file_data(folder=A_folder,file_names=A_murmur_files, duration=MAX_SOUND_CLIP_DURATION)\n",
        "A_murmur_labels = [1 for items in A_murmur_files]\n",
        "\n",
        "# test files\n",
        "A_unlabelledtest_files = fnmatch.filter(os.listdir(INPUT_DIR+'/set_a'), 'Aunlabelledtest*.wav')\n",
        "A_unlabelledtest_sounds = load_file_data(folder=A_folder,file_names=A_unlabelledtest_files, duration=MAX_SOUND_CLIP_DURATION)\n",
        "A_unlabelledtest_labels = [-1 for items in A_unlabelledtest_sounds]\n",
        "\n",
        "print (\"loaded dataset-a\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load file  /content/set_a/artifact__201106101955.wav\n",
            "fixing audio lenght : artifact__201106101955.wav\n",
            "load file  /content/set_a/artifact__201106131834.wav\n",
            "fixing audio lenght : artifact__201106131834.wav\n",
            "load file  /content/set_a/artifact__201106050353.wav\n",
            "fixing audio lenght : artifact__201106050353.wav\n",
            "load file  /content/set_a/artifact__201105060108.wav\n",
            "fixing audio lenght : artifact__201105060108.wav\n",
            "load file  /content/set_a/artifact__201106041452.wav\n",
            "fixing audio lenght : artifact__201106041452.wav\n",
            "load file  /content/set_a/artifact__201106211041.wav\n",
            "fixing audio lenght : artifact__201106211041.wav\n",
            "load file  /content/set_a/artifact__201106171003.wav\n",
            "fixing audio lenght : artifact__201106171003.wav\n",
            "load file  /content/set_a/artifact__201106190520.wav\n",
            "fixing audio lenght : artifact__201106190520.wav\n",
            "load file  /content/set_a/artifact__201106161019.wav\n",
            "fixing audio lenght : artifact__201106161019.wav\n",
            "load file  /content/set_a/artifact__201106111119.wav\n",
            "fixing audio lenght : artifact__201106111119.wav\n",
            "load file  /content/set_a/artifact__201106101314.wav\n",
            "fixing audio lenght : artifact__201106101314.wav\n",
            "load file  /content/set_a/artifact__201105040918.wav\n",
            "fixing audio lenght : artifact__201105040918.wav\n",
            "load file  /content/set_a/artifact__201106070537.wav\n",
            "fixing audio lenght : artifact__201106070537.wav\n",
            "load file  /content/set_a/artifact__201106040947.wav\n",
            "fixing audio lenght : artifact__201106040947.wav\n",
            "load file  /content/set_a/artifact__201106031558.wav\n",
            "fixing audio lenght : artifact__201106031558.wav\n",
            "load file  /content/set_a/artifact__201106030612.wav\n",
            "fixing audio lenght : artifact__201106030612.wav\n",
            "load file  /content/set_a/artifact__201106212112.wav\n",
            "fixing audio lenght : artifact__201106212112.wav\n",
            "load file  /content/set_a/artifact__201105061143.wav\n",
            "fixing audio lenght : artifact__201105061143.wav\n",
            "load file  /content/set_a/artifact__201105190800.wav\n",
            "fixing audio lenght : artifact__201105190800.wav\n",
            "load file  /content/set_a/artifact__201106211430.wav\n",
            "fixing audio lenght : artifact__201106211430.wav\n",
            "load file  /content/set_a/artifact__201106110909.wav\n",
            "fixing audio lenght : artifact__201106110909.wav\n",
            "load file  /content/set_a/artifact__201105041959.wav\n",
            "fixing audio lenght : artifact__201105041959.wav\n",
            "load file  /content/set_a/artifact__201106161219.wav\n",
            "fixing audio lenght : artifact__201106161219.wav\n",
            "load file  /content/set_a/artifact__201106040722.wav\n",
            "fixing audio lenght : artifact__201106040722.wav\n",
            "load file  /content/set_a/artifact__201106141701.wav\n",
            "fixing audio lenght : artifact__201106141701.wav\n",
            "load file  /content/set_a/artifact__201106220340.wav\n",
            "fixing audio lenght : artifact__201106220340.wav\n",
            "load file  /content/set_a/artifact__201106010559.wav\n",
            "fixing audio lenght : artifact__201106010559.wav\n",
            "load file  /content/set_a/artifact__201012172012.wav\n",
            "fixing audio lenght : artifact__201012172012.wav\n",
            "load file  /content/set_a/artifact__201106131835.wav\n",
            "fixing audio lenght : artifact__201106131835.wav\n",
            "load file  /content/set_a/artifact__201105280851.wav\n",
            "fixing audio lenght : artifact__201105280851.wav\n",
            "load file  /content/set_a/artifact__201106121242.wav\n",
            "fixing audio lenght : artifact__201106121242.wav\n",
            "load file  /content/set_a/artifact__201106070949.wav\n",
            "fixing audio lenght : artifact__201106070949.wav\n",
            "load file  /content/set_a/artifact__201106061233.wav\n",
            "fixing audio lenght : artifact__201106061233.wav\n",
            "load file  /content/set_a/artifact__201106121445.wav\n",
            "fixing audio lenght : artifact__201106121445.wav\n",
            "load file  /content/set_a/artifact__201106040933.wav\n",
            "fixing audio lenght : artifact__201106040933.wav\n",
            "load file  /content/set_a/artifact__201105051017.wav\n",
            "fixing audio lenght : artifact__201105051017.wav\n",
            "load file  /content/set_a/artifact__201106161016.wav\n",
            "fixing audio lenght : artifact__201106161016.wav\n",
            "load file  /content/set_a/artifact__201106021541.wav\n",
            "fixing audio lenght : artifact__201106021541.wav\n",
            "load file  /content/set_a/artifact__201106221254.wav\n",
            "fixing audio lenght : artifact__201106221254.wav\n",
            "load file  /content/set_a/artifact__201106010602.wav\n",
            "fixing audio lenght : artifact__201106010602.wav\n",
            "load file  /content/set_a/normal__201102201230.wav\n",
            "fixing audio lenght : normal__201102201230.wav\n",
            "load file  /content/set_a/normal__201106210943.wav\n",
            "fixing audio lenght : normal__201106210943.wav\n",
            "load file  /content/set_a/normal__201104141251.wav\n",
            "fixing audio lenght : normal__201104141251.wav\n",
            "load file  /content/set_a/normal__201102081152.wav\n",
            "fixing audio lenght : normal__201102081152.wav\n",
            "load file  /content/set_a/normal__201106221418.wav\n",
            "fixing audio lenght : normal__201106221418.wav\n",
            "load file  /content/set_a/normal__201103151912.wav\n",
            "fixing audio lenght : normal__201103151912.wav\n",
            "load file  /content/set_a/normal__201103221214.wav\n",
            "fixing audio lenght : normal__201103221214.wav\n",
            "load file  /content/set_a/normal__201108011114.wav\n",
            "fixing audio lenght : normal__201108011114.wav\n",
            "load file  /content/set_a/normal__201104122156.wav\n",
            "fixing audio lenght : normal__201104122156.wav\n",
            "load file  /content/set_a/normal__201105011626.wav\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGihjlMZ0-Rb"
      },
      "source": [
        "# load dataset-b, keep them separate for testing purpose \n",
        "B_folder=INPUT_DIR+'/set_b/'\n",
        "# set-b\n",
        "B_normal_files = fnmatch.filter(os.listdir(INPUT_DIR+'/set_b'), 'normal*.wav')  # include noisy files\n",
        "B_normal_sounds = load_file_data(folder=B_folder,file_names=B_normal_files, duration=MAX_SOUND_CLIP_DURATION)\n",
        "B_normal_labels = [2 for items in B_normal_sounds]\n",
        "\n",
        "B_murmur_files = fnmatch.filter(os.listdir(INPUT_DIR+'/set_b'), 'murmur*.wav')  # include noisy files\n",
        "B_murmur_sounds = load_file_data(folder=B_folder,file_names=B_murmur_files, duration=MAX_SOUND_CLIP_DURATION)\n",
        "B_murmur_labels = [1 for items in B_murmur_files]\n",
        "\n",
        "B_extrastole_files = fnmatch.filter(os.listdir(INPUT_DIR+'/set_b'), 'extrastole*.wav')\n",
        "B_extrastole_sounds = load_file_data(folder=B_folder,file_names=B_extrastole_files, duration=MAX_SOUND_CLIP_DURATION)\n",
        "B_extrastole_labels = [1 for items in B_extrastole_files]\n",
        "\n",
        "#test files\n",
        "B_unlabelledtest_files = fnmatch.filter(os.listdir(INPUT_DIR+'/set_b'), 'Bunlabelledtest*.wav')\n",
        "B_unlabelledtest_sounds = load_file_data(folder=B_folder,file_names=B_unlabelledtest_files, duration=MAX_SOUND_CLIP_DURATION)\n",
        "B_unlabelledtest_labels = [-1 for items in B_unlabelledtest_sounds]\n",
        "print (\"loaded dataset-b\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG1ZsZUC0-Rb"
      },
      "source": [
        "#combine set-a and set-b \n",
        "x_data = np.concatenate((A_artifact_sounds, A_normal_sounds,A_extrahls_sounds,A_murmur_sounds, \n",
        "                         B_normal_sounds,B_murmur_sounds,B_extrastole_sounds))\n",
        "\n",
        "y_data = np.concatenate((A_artifact_labels, A_normal_labels,A_extrahls_labels,A_murmur_labels,\n",
        "                         B_normal_labels,B_murmur_labels,B_extrastole_labels))\n",
        "\n",
        "test_x = np.concatenate((A_unlabelledtest_sounds,B_unlabelledtest_sounds))\n",
        "test_y = np.concatenate((A_unlabelledtest_labels,B_unlabelledtest_labels))\n",
        "\n",
        "print (\"combined training data record: \",len(y_data), len(test_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPgzCEFx0-Rb"
      },
      "source": [
        "y_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXUoCAJ-0-Rc"
      },
      "source": [
        "seed = 1000\n",
        "# split data into Train, Validation and Test\n",
        "x_train, x_test, y_train, y_test= train_test_split(x_data, y_data, train_size=0.8, random_state=seed, shuffle=True)\n",
        "#x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.8, random_state=seed, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FKU-G3-0-Rc"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjHk461S0-Rc"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPool1D, GlobalAvgPool1D, Dropout, BatchNormalization, Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
        "from keras.utils import np_utils\n",
        "from keras.regularizers import l2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtZdZZfE0-Rc"
      },
      "source": [
        "from scipy.signal import decimate\n",
        "x_train = decimate(x_train, 8, axis=1, zero_phase=True)\n",
        "x_train = decimate(x_train, 4, axis=1, zero_phase=True)\n",
        "#x_train = decimate(x_train, 4, axis=1, zero_phase=True)\n",
        "x_test = decimate(x_test, 8, axis=1, zero_phase=True)\n",
        "x_test = decimate(x_test, 4, axis=1, zero_phase=True)\n",
        "#x_test = decimate(x_test, 4, axis=1, zero_phase=True)\n",
        "\n",
        "#Scale each observation to unit variance, it should already have mean close to zero.\n",
        "#x_train = x_train / np.std(x_train, axis=1).reshape(-1,1)\n",
        "#x_test = x_test / np.std(x_test, axis=1).reshape(-1,1)\n",
        "\n",
        "x_train = x_train[:,:,np.newaxis]\n",
        "x_test = x_test[:,:,np.newaxis]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1O3-Wqn0-Rc"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWhyMeJC0-Rc"
      },
      "source": [
        "#new_labels = np.array(new_labels, dtype='int')\n",
        "Y_train = np_utils.to_categorical(y_train)\n",
        "Y_test=np_utils.to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC_m8YNA0-Rc"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(filters=4, kernel_size=9, activation='relu', input_shape = x_train.shape[1:],kernel_regularizer = l2(0.025)))\n",
        "model.add(MaxPool1D(strides=4))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv1D(filters=4, kernel_size=(9), activation='relu',\n",
        "                kernel_regularizer = l2(0.05)))\n",
        "model.add(MaxPool1D(strides=4))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv1D(filters=8, kernel_size=(9), activation='relu',\n",
        "                 kernel_regularizer = l2(0.1)))\n",
        "model.add(MaxPool1D(strides=4))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv1D(filters=16, kernel_size=(9), activation='relu'))\n",
        "model.add(MaxPool1D(strides=4))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv1D(filters=64, kernel_size=(4), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv1D(filters=32, kernel_size=(1), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.75))\n",
        "model.add(GlobalAvgPool1D())\n",
        "model.add(Dense(3, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNuqpIyc0-Rd"
      },
      "source": [
        "def batch_generator(x_train, y_train, batch_size):\n",
        "    \"\"\"\n",
        "    Rotates the time series randomly in time\n",
        "    \"\"\"\n",
        "    x_batch = np.empty((batch_size, x_train.shape[1], x_train.shape[2]), dtype='float32')\n",
        "    y_batch = np.empty((batch_size, y_train.shape[1]), dtype='float32')\n",
        "    full_idx = range(x_train.shape[0])\n",
        "    \n",
        "    while True:\n",
        "        batch_idx = np.random.choice(full_idx, batch_size)\n",
        "        x_batch = x_train[batch_idx]\n",
        "        y_batch = y_train[batch_idx]\n",
        "    \n",
        "        for i in range(batch_size):\n",
        "            sz = np.random.randint(x_batch.shape[1])\n",
        "            x_batch[i] = np.roll(x_batch[i], sz, axis = 0)\n",
        "     \n",
        "        yield x_batch, y_batch\n",
        "        \n",
        "weight_saver = ModelCheckpoint('set_a_weights.h5', monitor='val_loss', \n",
        "                               save_best_only=True, save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyg0zzn70-Rd"
      },
      "source": [
        "model.compile(optimizer=Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.8**x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGqx4WYC0-Rd"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZfFTSiu0-Rd"
      },
      "source": [
        "hist = model.fit_generator(batch_generator(x_train, Y_train, 8),\n",
        "                   epochs=10, steps_per_epoch=1000,\n",
        "                   validation_data=(x_test, Y_test),\n",
        "                   callbacks=[weight_saver, annealer],\n",
        "                   verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-eDev0B0-Rd"
      },
      "source": [
        "model.load_weights('set_a_weights.h5')\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(hist.history['loss'], color='b')\n",
        "plt.plot(hist.history['val_loss'], color='r')\n",
        "plt.show()\n",
        "plt.plot(hist.history['acc'], color='b')\n",
        "plt.plot(hist.history['val_acc'], color='r')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opV3b6NK0-Rd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f9gjzTv0-Rd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijRbvhdm0-Rd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}